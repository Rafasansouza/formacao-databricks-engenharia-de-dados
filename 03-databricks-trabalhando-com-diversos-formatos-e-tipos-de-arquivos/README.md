# Manipula√ß√£o de Arquivos com PySpark no Databricks

Este reposit√≥rio apresenta as pr√°ticas desenvolvidas durante o curso de Engenharia de Dados, com foco na **leitura, escrita, compress√£o e particionamento de arquivos** utilizando **PySpark** no **Databricks**.

## T√≥picos aprendidos

### üìÅ Trabalhando com DBFS e dbutils
- Cria√ß√£o de diret√≥rios no Databricks File System (DBFS) usando `dbutils.fs.mkdirs`.
- Listagem de arquivos e diret√≥rios com `dbutils.fs.ls`.
- Carregamento de arquivos JSON para o Spark.

### üìÑ Manipula√ß√£o de DataFrames
- Renomea√ß√£o de colunas para nomes mais descritivos usando `withColumnRenamed`.
- Filtragem de dados inv√°lidos.
- Convers√£o de tipos de dados utilizando `cast` do `pyspark.sql.functions`.

### üóúÔ∏è Escrita e compress√£o de arquivos
- Escrita de arquivos JSON com compress√£o GZIP.
- Escrita de arquivos CSV com e sem cabe√ßalho e aplica√ß√£o de compress√£o GZIP.
- Escrita de arquivos de texto (`TXT`), separando colunas com delimitador `|` e compactando em GZIP.

### üßπ Limpeza e padroniza√ß√£o dos dados
- Substitui√ß√£o de valores nulos em colunas espec√≠ficas utilizando `na.fill`.
- Releitura de arquivos ap√≥s compress√£o e renomea√ß√£o adequada das colunas.

### üì¶ Trabalho com formatos AVRO
- Escrita de dados no formato AVRO.
- Aplica√ß√£o de compress√£o no AVRO utilizando o codec `deflate`.
- Configura√ß√µes avan√ßadas como n√≠vel de compress√£o no AVRO (`spark.sql.avro.deflate.level`).

### üìä Trabalho com formato PARQUET
- Escrita de dados no formato Parquet.
- Aplica√ß√£o de compress√£o GZIP em arquivos Parquet.
- Leitura e exibi√ß√£o dos dados Parquet.
- Uso de `select().distinct().show()` para explora√ß√£o de valores √∫nicos em colunas espec√≠ficas.

### üî• Particionamento de dados
- Particionamento dos dados Parquet por uma ou mais colunas (ex.: `nivel_territorial` e `cod_doenca`).
- Leitura seletiva de parti√ß√µes espec√≠ficas.

### üìÇ Trabalho com formato ORC
- Escrita de dados em formato ORC.
- Aplica√ß√£o de compress√£o `zlib` no ORC.
- Uso de `coalesce(1)` para unir os arquivos ORC em um √∫nico arquivo comprimido.

## üìö Resumo das tecnologias e formatos utilizados
- **DBFS**: Armazenamento de arquivos no Databricks.
- **dbutils**: Utilit√°rios para manipula√ß√£o de diret√≥rios e arquivos no Databricks.
- **PySpark**: API Python para manipula√ß√£o de Big Data distribu√≠do.
- **JSON**: Leitura, escrita e compress√£o.
- **CSV**: Leitura, escrita, compress√£o e infer√™ncia de esquema.
- **TXT**: Escrita com delimitador customizado e compress√£o.
- **AVRO**: Escrita e compress√£o de arquivos compactos e esquematizados.
- **PARQUET**: Escrita, compress√£o e particionamento para otimiza√ß√£o de leitura.
- **ORC**: Escrita e compress√£o eficiente de arquivos em Big Data.

---

## üöÄ Sobre o aprendizado
Durante a pr√°tica, pudemos compreender:
- A diferen√ßa de formatos de arquivos e compress√µes.
- A import√¢ncia de tipos corretos de dados para efici√™ncia.
- Como o particionamento afeta a performance em grandes volumes de dados.
- Como manipular diferentes tipos de arquivos para preparar os dados para an√°lises.

---

> ‚ö° Este projeto refor√ßou os conceitos fundamentais de manipula√ß√£o de dados no Databricks usando PySpark, consolidando as melhores pr√°ticas para lidar com grandes volumes de dados em diferentes formatos e compress√µes.